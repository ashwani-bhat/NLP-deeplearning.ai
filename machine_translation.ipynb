{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine_translation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMQBo3PxsQmf/ESlw3mNcO7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwani-bhat/NLP-deeplearning.ai/blob/master/machine_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av4UBJ0jPlSd",
        "colab_type": "text"
      },
      "source": [
        "#Downloads and headers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QaAowhk7-jh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b07ed6a7-8b26-406d-f88e-198a78e2741e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2vC_l1EPyw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pdb\n",
        "import pickle\n",
        "import string\n",
        "\n",
        "import time\n",
        "\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import scipy\n",
        "import sklearn\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUk_kt8VRThX",
        "colab_type": "text"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruToaJmgP-ZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    '''\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "            word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean\n",
        "\n",
        "\n",
        "def get_dict(file_name):\n",
        "    \"\"\"\n",
        "    This function returns the english to french dictionary given a file where the each column corresponds to a word.\n",
        "    Check out the files this function takes in your workspace.\n",
        "    \"\"\"\n",
        "    my_file = pd.read_csv(file_name, delimiter=' ')\n",
        "    etof = {}  # the english to french dictionary to be returned\n",
        "    for i in range(len(my_file)):\n",
        "        # indexing into the rows.\n",
        "        en = my_file.loc[i][0]\n",
        "        fr = my_file.loc[i][1]\n",
        "        etof[en] = fr\n",
        "\n",
        "    return etof\n",
        "\n",
        "\n",
        "def cosine_similarity(A, B):\n",
        "    '''\n",
        "    Input:\n",
        "        A: a numpy array which corresponds to a word vector\n",
        "        B: A numpy array which corresponds to a word vector\n",
        "    Output:\n",
        "        cos: numerical number representing the cosine similarity between A and B.\n",
        "    '''\n",
        "    # you have to set this variable to the true label.\n",
        "    cos = -10\n",
        "    dot = np.dot(A, B)\n",
        "    norma = np.linalg.norm(A)\n",
        "    normb = np.linalg.norm(B)\n",
        "    cos = dot / (norma * normb)\n",
        "\n",
        "    return cos\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn6h-rULRbR7",
        "colab_type": "text"
      },
      "source": [
        "#Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEml_C5yRaWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cwd = 'Natural Language Processing with Classification and Vector Spaces/Week 4/'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_X8GCA9h3ap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_embeddings_subset = pickle.load(open(cwd + \"en_embeddings.p\", \"rb\"))\n",
        "fr_embeddings_subset = pickle.load(open(cwd + \"fr_embeddings.p\", \"rb\"))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnHjA0EbiAB0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "99c63cb8-d536-479c-9f75-c81c7b17a91e"
      },
      "source": [
        "# loading the english to french dictionaries\n",
        "en_fr_train = get_dict(cwd + 'en-fr.train.txt')\n",
        "print('The length of the English to French training dictionary is', len(en_fr_train))\n",
        "en_fr_test = get_dict(cwd + 'en-fr.test.txt')\n",
        "print('The length of the English to French test dictionary is', len(en_fr_train))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of the English to French training dictionary is 5000\n",
            "The length of the English to French test dictionary is 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0VPq0h_iZOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_matrices(en_fr, french_vecs, english_vecs):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        en_fr: English to French dictionary\n",
        "        french_vecs: French words to their corresponding word embeddings.\n",
        "        english_vecs: English words to their corresponding word embeddings.\n",
        "    Output: \n",
        "        X: a matrix where the columns are the English embeddings.\n",
        "        Y: a matrix where the columns correspong to the French embeddings.\n",
        "        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n",
        "    \"\"\"\n",
        "    # X_l and Y_l are lists of the english and french word embeddings\n",
        "    X_l = list()\n",
        "    Y_l = list()\n",
        "\n",
        "    # get the english words (the keys in the dictionary) and store in a set()\n",
        "    english_set = set(english_vecs.keys())\n",
        "\n",
        "    # get the french words (keys in the dictionary) and store in a set()\n",
        "    french_set = set(french_vecs.keys())\n",
        "\n",
        "    # store the french words that are part of the english-french dictionary (these are the values of the dictionary)\n",
        "    french_words = set(en_fr.values())\n",
        "\n",
        "    # loop through all english, french word pairs in the english french dictionary\n",
        "    for en_word, fr_word in en_fr.items():\n",
        "\n",
        "        # check that the french word has an embedding and that the english word has an embedding\n",
        "        if fr_word in french_set and en_word in english_set:\n",
        "\n",
        "            # get the english embedding\n",
        "            en_vec = english_vecs[en_word]\n",
        "\n",
        "            # get the french embedding\n",
        "            fr_vec = french_vecs[fr_word]\n",
        "\n",
        "            # add the english embedding to the list\n",
        "            X_l.append(en_vec)\n",
        "\n",
        "            # add the french embedding to the list\n",
        "            Y_l.append(fr_vec)\n",
        "\n",
        "    # stack the vectors of X_l into a matrix X\n",
        "    X = np.vstack(X_l)\n",
        "\n",
        "    # stack the vectors of Y_l into a matrix Y\n",
        "    Y = np.vstack(Y_l)\n",
        "\n",
        "    return X, Y\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu0O1uJgimJ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "67e3c18f-f038-4f4e-a60d-db254ba84aaa"
      },
      "source": [
        "# getting the training set:\n",
        "X_train, Y_train = get_matrices(\n",
        "    en_fr_train, fr_embeddings_subset, en_embeddings_subset)\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4932, 300)\n",
            "(4932, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG19bhFHiyHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_loss(X, Y, R):\n",
        "    '''\n",
        "    Inputs: \n",
        "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
        "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
        "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
        "    Outputs:\n",
        "        L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.\n",
        "    '''\n",
        "    # m is the number of rows in X\n",
        "    m = X.shape[0]\n",
        "    \n",
        "    # diff is XR - Y\n",
        "    diff = np.dot(X, R) - Y\n",
        "\n",
        "    # diff_squared is the element-wise square of the difference\n",
        "    diff_squared = diff * diff\n",
        "\n",
        "    # sum_diff_squared is the sum of the squared elements\n",
        "    sum_diff_squared = np.sum(diff_squared)\n",
        "\n",
        "    # loss i the sum_diff_squard divided by the number of examples (m)\n",
        "    loss = sum_diff_squared / m\n",
        "    return loss\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVwVfD2Ii492",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_gradient(X, Y, R):\n",
        "    '''\n",
        "    Inputs: \n",
        "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
        "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
        "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
        "    Outputs:\n",
        "        g: a matrix of dimension (n,n) - gradient of the loss function L for given X, Y and R.\n",
        "    '''\n",
        "    # m is the number of rows in X\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # gradient is X^T(XR - Y) * 2/m\n",
        "    gradient = (2/m) * (np.dot(X.T, np.dot(X, R) - Y))\n",
        "    return gradient\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NpRGXVMi-wA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003):\n",
        "    '''\n",
        "    Inputs:\n",
        "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
        "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
        "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
        "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
        "    Outputs:\n",
        "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
        "    '''\n",
        "    np.random.seed(129)\n",
        "\n",
        "    # the number of columns in X is the number of dimensions for a word vector (e.g. 300)\n",
        "    # R is a square matrix with length equal to the number of dimensions in th  word embedding\n",
        "    R = np.random.rand(X.shape[1], X.shape[1])\n",
        "\n",
        "    for i in range(train_steps):\n",
        "        if i % 25 == 0:\n",
        "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
        "        # use the function that you defined to compute the gradient\n",
        "        gradient = compute_gradient(X, Y, R)\n",
        "\n",
        "        # update R by subtracting the learning rate times gradient\n",
        "        R -= learning_rate * gradient\n",
        "    return R\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIB4dcQJjGL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "8e4233b4-b7c5-4994-e599-4a22bb19072c"
      },
      "source": [
        "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss at iteration 0 is: 963.0146\n",
            "loss at iteration 25 is: 97.8292\n",
            "loss at iteration 50 is: 26.8329\n",
            "loss at iteration 75 is: 9.7893\n",
            "loss at iteration 100 is: 4.3776\n",
            "loss at iteration 125 is: 2.3281\n",
            "loss at iteration 150 is: 1.4480\n",
            "loss at iteration 175 is: 1.0338\n",
            "loss at iteration 200 is: 0.8251\n",
            "loss at iteration 225 is: 0.7145\n",
            "loss at iteration 250 is: 0.6534\n",
            "loss at iteration 275 is: 0.6185\n",
            "loss at iteration 300 is: 0.5981\n",
            "loss at iteration 325 is: 0.5858\n",
            "loss at iteration 350 is: 0.5782\n",
            "loss at iteration 375 is: 0.5735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2rnpNVZjNff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nearest_neighbor(v, candidates, k=1):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - v, the vector you are going find the nearest neighbor for\n",
        "      - candidates: a set of vectors where we will find the neighbors\n",
        "      - k: top k nearest neighbors to find\n",
        "    Output:\n",
        "      - k_idx: the indices of the top k closest vectors in sorted form\n",
        "    \"\"\"\n",
        "    similarity_l = []\n",
        "\n",
        "    # for each candidate vector...\n",
        "    for row in candidates:\n",
        "        # get the cosine similarity\n",
        "        cos_similarity = cosine_similarity(row, v)\n",
        "\n",
        "        # append the similarity to the list\n",
        "        similarity_l.append(cos_similarity)\n",
        "        \n",
        "    # sort the similarity list and get the indices of the sorted list\n",
        "    sorted_ids = np.argsort(similarity_l)[::-1]\n",
        "\n",
        "    # get the indices of the k most similar candidate vectors\n",
        "    k_idx = sorted_ids[:k]\n",
        "    return k_idx\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W91h3Y08jS-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_vocabulary(X, Y, R):\n",
        "    '''\n",
        "    Input:\n",
        "        X: a matrix where the columns are the English embeddings.\n",
        "        Y: a matrix where the columns correspong to the French embeddings.\n",
        "        R: the transform matrix which translates word embeddings from\n",
        "        English to French word vector space.\n",
        "    Output:\n",
        "        accuracy: for the English to French capitals\n",
        "    '''\n",
        "    # The prediction is X times R\n",
        "    pred = np.dot(X, R)\n",
        "\n",
        "    # initialize the number correct to zero\n",
        "    num_correct = 0\n",
        "\n",
        "    # loop through each row in pred (each transformed embedding)\n",
        "    for i in range(len(pred)):\n",
        "        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\n",
        "        pred_idx = nearest_neighbor(pred[i], Y, 1)\n",
        "\n",
        "        # if the index of the nearest neighbor equals the row of i... \\\n",
        "        if pred_idx == i:\n",
        "            # increment the number correct by 1.\n",
        "            num_correct += 1\n",
        "\n",
        "    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\n",
        "    accuracy = num_correct / X.shape[0]\n",
        "\n",
        "    return accuracy\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIG5mN5SjcoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oun29FCwjfBI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e42dc616-65af-460d-9371-f5c74915c22e"
      },
      "source": [
        "acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\n",
        "print(f\"accuracy on test set is {acc:.3f}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy on test set is 0.557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o5zMGTxjxR_",
        "colab_type": "text"
      },
      "source": [
        "#Locality sensitive hashing and document search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGlfxoShjjTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}